{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9af993-8586-4300-9182-c897607dcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating imbalanced model:\n",
      "\n",
      "Imbalanced MLP - Fold 1 Results:\n",
      "Accuracy: 0.9195\n",
      "Sensitivity: 0.5841\n",
      "Specificity: 0.6444\n",
      "F1 Score: 0.9165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1440/2667551215.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  specificity = np.diag(cm) / np.sum(cm, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imbalanced MLP - Fold 2 Results:\n",
      "Accuracy: 0.9343\n",
      "Sensitivity: 0.6679\n",
      "Specificity: nan\n",
      "F1 Score: 0.9332\n",
      "\n",
      "Imbalanced MLP - Fold 3 Results:\n",
      "Accuracy: 0.9300\n",
      "Sensitivity: 0.6534\n",
      "Specificity: 0.7028\n",
      "F1 Score: 0.9282\n",
      "\n",
      "Imbalanced MLP - Fold 4 Results:\n",
      "Accuracy: 0.9284\n",
      "Sensitivity: 0.6524\n",
      "Specificity: 0.6996\n",
      "F1 Score: 0.9271\n",
      "\n",
      "Imbalanced MLP - Average Cross-Validation Results:\n",
      "Accuracy: 0.9281\n",
      "Sensitivity: 0.6394\n",
      "Specificity: nan\n",
      "F1: 0.9263\n",
      "\n",
      "Training and evaluating balanced model with oversampling:\n",
      "\n",
      "Balanced MLP (Oversampling) - Fold 1 Results:\n",
      "Accuracy: 0.9211\n",
      "Sensitivity: 0.6501\n",
      "Specificity: 0.6859\n",
      "F1 Score: 0.9202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set project root directory\n",
    "PROJECT_ROOT = '/mnt/c/Users/Linda/Desktop/GitHub_Projects/MLP_ProteinLocalization'\n",
    "\n",
    "# Load data\n",
    "features = pd.read_csv(os.path.join(PROJECT_ROOT, 'data', 'features.csv'))\n",
    "labels = pd.read_csv(os.path.join(PROJECT_ROOT, 'data', 'labels.csv'))\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels['Type'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    specificity = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, sensitivity, specificity, f1, cm\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    os.makedirs(os.path.join(PROJECT_ROOT, 'results'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(PROJECT_ROOT, 'results', f\"{title.lower().replace(' ', '_')}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(X, y, model_name, sampling_method=None):\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    cv_scores = {'accuracy': [], 'sensitivity': [], 'specificity': [], 'f1': []}\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "        \n",
    "        if sampling_method == 'oversample':\n",
    "            sampler = SMOTE(random_state=42)\n",
    "            X_train_fold, y_train_fold = sampler.fit_resample(X_train_fold, y_train_fold)\n",
    "        elif sampling_method == 'undersample':\n",
    "            sampler = RandomUnderSampler(random_state=42)\n",
    "            X_train_fold, y_train_fold = sampler.fit_resample(X_train_fold, y_train_fold)\n",
    "        \n",
    "        model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', solver='adam', alpha=0.001, learning_rate='adaptive', max_iter=1000, batch_size=64, early_stopping=True, validation_fraction=0.2, random_state=42)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        accuracy, sensitivity, specificity, f1, _ = evaluate_model(y_val_fold, y_pred)\n",
    "        cv_scores['accuracy'].append(accuracy)\n",
    "        cv_scores['sensitivity'].append(np.mean(sensitivity))\n",
    "        cv_scores['specificity'].append(np.mean(specificity))\n",
    "        cv_scores['f1'].append(f1)\n",
    "        \n",
    "        print(f\"\\n{model_name} - Fold {fold} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Sensitivity: {np.mean(sensitivity):.4f}\")\n",
    "        print(f\"Specificity: {np.mean(specificity):.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Average scores across folds\n",
    "    avg_scores = {metric: np.mean(cv_scores[metric]) for metric in cv_scores}\n",
    "    print(f\"\\n{model_name} - Average Cross-Validation Results:\")\n",
    "    for metric, avg in avg_scores.items():\n",
    "        print(f\"{metric.capitalize()}: {avg:.4f}\")\n",
    "    return avg_scores\n",
    "\n",
    "# Function to plot feature importance\n",
    "def plot_feature_importance(model, X, y, feature_names, title):\n",
    "    result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': result.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PROJECT_ROOT, 'results', f\"{title.lower().replace(' ', '_')}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Test model function\n",
    "def test_model(X_train, y_train, X_test, y_test, model_name, sampling_method=None):\n",
    "    if sampling_method == 'oversample':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    elif sampling_method == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', solver='adam', alpha=0.001, learning_rate='adaptive', max_iter=1000, batch_size=64, early_stopping=True, validation_fraction=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy, sensitivity, specificity, f1, cm = evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} - Test Set Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Sensitivity: {np.mean(sensitivity):.4f}\")\n",
    "    print(f\"Specificity: {np.mean(specificity):.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    plot_confusion_matrix(cm, le.classes_, f\"{model_name} Test Set Confusion Matrix\")\n",
    "    plot_feature_importance(model, X_test, y_test, features.columns, f\"{model_name} Feature Importance\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train and evaluate on imbalanced data\n",
    "print(\"Training and evaluating imbalanced model:\")\n",
    "imbalanced_avg = train_and_evaluate(X_train, y_train, \"Imbalanced MLP\")\n",
    "\n",
    "# Train and evaluate with oversampling\n",
    "print(\"\\nTraining and evaluating balanced model with oversampling:\")\n",
    "balanced_over_avg = train_and_evaluate(X_train, y_train, \"Balanced MLP (Oversampling)\", sampling_method='oversample')\n",
    "\n",
    "# Train and evaluate with undersampling\n",
    "print(\"\\nTraining and evaluating balanced model with undersampling:\")\n",
    "balanced_under_avg = train_and_evaluate(X_train, y_train, \"Balanced MLP (Undersampling)\", sampling_method='undersample')\n",
    "\n",
    "# Test each model\n",
    "imbalanced_model = test_model(X_train, y_train, X_test, y_test, \"Imbalanced MLP\")\n",
    "oversampled_model = test_model(X_train, y_train, X_test, y_test, \"Balanced MLP (Oversampling)\", sampling_method='oversample')\n",
    "undersampled_model = test_model(X_train, y_train, X_test, y_test, \"Balanced MLP (Undersampling)\", sampling_method='undersample')\n",
    "\n",
    "print(\"\\nConfusion matrices for test results and feature importance plots have been saved in the results folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3140abe-36cb-4b4d-b35d-7fe7c9d5fd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
